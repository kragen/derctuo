I was reading over Darius Bacon's [The Language of Choice][0]
yesterday, which is an introduction to binary decision diagrams;
chatting with Bacon, a couple of ideas came out that I thought I ought
to write down.

(I'm going to ignore tokenization and thus whitespace here, since it
would add more heat than light.)

[0]: https://codewords.recurse.com/issues/four/the-language-of-choice

One is that the language generated by the CFG

    <expr> ::= <var> | <expr> if <expr> else <expr>

is context-free, but left-recursive and ambiguous (`a if b else c if d
else e` can be parsed as testing `b` first or `d` first).  If you want
to parse it using a standard linear-time Packrat parser, you need to
factor out the left-recursion and resolve the ambiguity.  Bacon
suggested two ways of doing this, one right-associative as it should
be:

    <expr> ← <var> (if <expr> else <expr>)?

and the other left-associative:

    <expr> ← <var> (if <expr> else <var>)*

Of course, once you have a parse tree, it's a simple pattern-matching
exercise to construct a parse tree with a different associativity.

XXX verify these

In this form the language is missing a little bit of expressivity; it
can only express an unbranching chain of single-variable choices, and
it can't express negation.  You can enhance this with nesting to be
able to handle arbitrary boolean functions:

    <expr> ← ("(" <expr> ")" / <var> / <const>) (if <expr> else <expr>)?
    <const> ← 0 / 1

I was thinking that this grammar might be a good example of when you
benefit from Packrat's memoization, but in fact it's not, once the
left recursion is factored out.  I'd like to figure out how to work
this out rigorously: how can we know that a given memo-table entry can
never be used?  Is there a single canonical answer that's practical to
compute, or is it more a question of a series of conservative
approximations, progressively less simple?

Perhaps in this case we can observe that no retry-after-backtrack of
any callsite of `<expr>` will ever invoke `<expr>` a second time.
This is a whole-grammar property because it flows through invocations;
we could falsify it for both `<expr>` and `<var>` by, for example,
adding this rule to the above grammar:

    <when> ← <expr> when <expr> / <expr>

But if we refactor that rule as follows, we regain the desirable
memorylessness property:

    <when> ← <expr> (when <expr>)?

There's a trick due to, I think, Warth, which lets Packrat parse some
left-recursive grammars at the expense of their linear-time guarantee,
which I think would enable Packrat to handle the original grammar in
PEG form:

    <expr> ← <expr> if <expr> else <expr> / <var>

When it detects a left-recursive call, it initially fails; if it
finishes parsing the rule successfully, for example with the `<var>`
alternative here, it enters the parse result into the memo table, then
restarts parsing, this time allowing the left-recursive call to
succeed.  So, for example, parsing `a if b else c if d else e`, it
would initially parse `a` with the second alternative, and then on a
second go-round, use that `a` for the left-recursive `<expr>`
invocation and succeed in parsing `a if b else c`, which would replace
`a` in the memo table; after restarting a second time, the initial
left-recursive `<expr>` requirement would be fulfilled with the whole
`a if b else c` from the previous attempt, and the whole expression
would be parsed as an `<expr>`, though associating incorrectly.

After a third restart, an attempt to parse an `<expr>` that begins
with `a if b else c if d else e` fails, because that is followed by
end of input, not by `if`.  So we keep the former parse.

Essentially, this left-recursion trick converts a Packrat parser
locally from a top-down parser into a bottom-up parser.

----

An earlier draft of Bacon's article, if I recall correctly, chose the
symbols `←` and `→` rather than `0` and `1` for the Boolean values,
and used infix rather than prefix syntax, so (if we interpret `←` as
True and `→` as False) `a if b else c` would be written `a {b} c`,
which is `a` if `b` evaluates to `←` and `c` if `b` evaluates to `→`.
That is, `a {←} c` evaluates to `a`, and `a {→} c` evaluates to `c`.
So you got nice identities like `{0 {x} 1} = x`, `a {1 {x} 0} b = b
{x} a`, and `a {b} a = a`.  I really liked this idea, but he dropped
it for the final version of the article as making it less accessible.

Laws of Form
------------

The other clever Boolean notation idea is the one from Laws of Form,
which is optimized for handwriting rather than strings of characters;
in a string-of-characters form we can use nested parentheses, which
indicate negation.  One interpretation is that in LoF juxtaposition
represents OR and the empty string represents False, so we can write
the whole canonical evaluation ruleset as follows:

    ()() -> ()
    (()) ->

That is, two empty sets of parentheses ("crosses" in LoF jargon)
juxtaposed can be rewritten as a single empty set of parentheses, and
a parenthesized empty parenthesis can be rewritten as an empty string.

This is essentially the same as the Sheffer (spelling?) stroke or NOR
logic, but with a notation that exploits a homeomorphism between the
empty string being the identity element in the free monad and
falsehood being the identity element for Boolean OR.  (LoF itself is
silent on the correspondence between its crosses and logic, so you can
equally well interpret the empty string as true and juxtaposition as
AND.)

While the notation is pleasantly straightforward for explicit
evaluation, my attempts to formulate normalization rules such as CNF
normalization as tree rewriting rules for LoF notation were not very
fruitful — they seemed to come out a great deal less clear than in
traditional Boolean logic, and at the time I wasn't able to get them
working.

